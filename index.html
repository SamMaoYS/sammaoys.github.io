<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
  <meta charset="UTF-8" />
  <title>Yongsen Mao</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,600i%7CSource+Code+Pro:400,400i,600" />
  <link rel="stylesheet" href="/static/m-dark.css" />
  <link rel="icon" href="/favicon.ico" type="image/x-ico" />
  <link rel="canonical" href="/" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#22272e" />
  <meta name="description" content="I graduated as a thesis-based master student at SFU (Simon Fraser University), specializing in the fields of 3D Computer Vision and Graphics." />
  <meta property="og:site_name" content="Yongsen Mao" />
  <meta property="og:title" content="Yongsen Mao" />
  <meta name="twitter:title" content="Yongsen Mao" />
  <meta property="og:url" content="/" />
  <meta property="og:description" content="I graduated as a thesis-based master student at SFU (Simon Fraser University), specializing in the fields of 3D Computer Vision and Graphics." />
  <meta name="twitter:description" content="I graduated as a thesis-based master student at SFU (Simon Fraser University), specializing in the fields of 3D Computer Vision and Graphics." />
  <meta name="twitter:card" content="summary" />
  <meta property="og:type" content="page" />
</head>
<body>
<header><nav id="navigation">
  <div class="m-container">
    <div class="m-row">
      <a href="/" id="m-navbar-brand" class="m-col-t-9 m-col-m-none m-left-m">Yongsen Mao</a>
      <a id="m-navbar-show" href="#navigation" title="Show navigation" class="m-col-t-3 m-hide-m m-text-right"></a>
      <a id="m-navbar-hide" href="#" title="Hide navigation" class="m-col-t-3 m-hide-m m-text-right"></a>
      <div id="m-navbar-collapse" class="m-col-t-12 m-show-m m-col-m-none m-right-m">
        <div class="m-row">
          <ol class="m-col-t-12 m-col-m-none">
            <li><a href="/">Home</a></li>
            <li><a href="/images/YSM_CV.pdf">CV</a></li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</nav></header>
<main>
<article>
  <div id="m-landing-image">
    <div id="m-landing-cover">
      <div class="m-container">
<!-- landing -->
<div class="m-container">
<div class="m-row">
<div class="m-col-l-6">
<img alt="Yongsen Mao" class="m-image" src="/images/yongsen_avatar.jpg" style="width: 50%" />
</div>
<div class="m-col-l-6">
<h1 style="text-transform: capitalize;">Yongsen Mao</h1>

<div>I graduated as a thesis-based master student at <a href="https://www.sfu.ca" class="m-link-wrap">SFU</a> (Simon Fraser University), specializing in the fields of 3D Computer Vision and Graphics. I am fortunate to be supervised by Professors <a href="https://msavva.github.io" class="m-link-wrap">Manolis Savva</a> and mentored by <a href="https://angelxuanchang.github.io" class="m-link-wrap">Angel Xuan Chang</a> in the <a href="https://gruvi.cs.sfu.ca" class="m-link-wrap">GrUVi Lab</a>. My primary interest lies in the generation and understanding of 3D scenes for downstream vision and robotics applications. Prior to this, I received B.Eng. from <a href="https://www.zju.edu.cn/english" class="m-link-wrap">ZJU</a> (Zhejiang University) and SFU.</div>

<br/>
<br/>

<div>
sammaoys-{at}-outlook-[dot]-com&emsp;
<a href="https://scholar.google.com/citations?user=bm9JqwMAAAAJ&hl=en" class="m-link-wrap">Google Scholar</a>&emsp;
<a href="https://github.com/SamMaoYS" class="m-link-wrap">GitHub</a>
</div></div>
</div>
</div>
<!-- /landing -->
      </div>
    </div>
  </div>
  <div class="m-container m-container-inflatable">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
<!-- content -->
<section id="news">
<h2>News</h2>
<div class="m-container">
<div class="m-row">
<div>
    <p>2025/06 We released <a href="https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B">SpatialLM 1.1</a>, an improved version of SpatialLM.</p>
    <p>2025/03 We started a new open-source project: <a href="https://manycore-research.github.io/SpatialLM">SpatialLM</a>.</p>
    <p>2024/06 Our layout controlnet model released on huggingface <a href="https://huggingface.co/kujiale-ai/controlnet-layout">kujiale-ai/controlnet-layout</a>, and demo at <a href="https://huggingface.co/spaces/ysmao/Layout-Control">ysmao/Layout-Control</a> </p>
    <p>2024/05 Our paper on Ego-Exo4D is accepted to CVPR 2024. </p>
    <p>2024/05 Our paper on HSSD-200 is accepted to CVPR 2024. </p>
    <p>2023/11 Joined <a href="https://www.kujiale.com/">KuJiaLe</a>/<a href="https://www.coohom.com"> Coohom </a> as a research engineer. </p>
</div></div>
</div>
</section>
<section id="publications">
<h2>Publications</h2>
<div class="m-row m-block m-primary">
<div class="m-col-l-4">
<img alt="spatiallm" class="m-image" src="/images/papers/spatiallm.jpg" />
</div>
<div class="m-col-l-8">
<h3>SpatialLM: Training Large Language Models for Structured Indoor Modeling</h3>

<div class="m-text">
    Yongsen Mao*, <a>Junhao Zhong</a>*, <a href="https://fangchuan.github.io/">Chuan Fang</a>, <a href="https://bertjiazheng.github.io">Jia Zheng</a>, <a>Rui Tang</a>, <a>Hao Zhu</a>, <a href="https://pingtan.people.ust.hk/index.html">Ping Tan</a>, <a href="https://zihan-z.github.io/">Zihan Zhou</a>
</div>
<br/>

<div class="m-text">
SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs. To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.
</div>

<br/>

<div class="m-text">arXiv</div>

<div class="m-text">
<a href="https://arxiv.org/abs/2506.07491" class="m-link-wrap">Paper</a>, <a href="https://manycore-research.github.io/SpatialLM/" class="m-link-wrap">Project</a>, <a href="https://github.com/manycore-research/SpatialLM" class="m-link-wrap">Code</a>
</div></div>
</div>
<div class="m-row m-block m-primary">
<div class="m-col-l-4">
<img alt="egoexo4d" class="m-image" src="/images/papers/egoexo4d.jpeg" />
</div>
<div class="m-col-l-8">
<h3>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</h3>

<div class="m-text">
<a>Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder,</a> Yongsen Mao <a>, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray</a>
</div>
<br/>

<div class="m-text">
We present Ego-Exo4D, a diverse, large-scale multimodal multiview video dataset and benchmark challenge. Ego-Exo4D centers around simultaneously-captured egocentric and exocentric video of skilled human activities (e.g., sports, music, dance, bike repair). 740 participants from 13 cities worldwide performed these activities in 123 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,286 hours of video combined. The multimodal nature of the dataset is unprecedented: the video is accompanied by multichannel audio, eye gaze, 3D point clouds, camera poses, IMU, and multiple paired language descriptions -- including a novel "expert commentary" done by coaches and teachers and tailored to the skilled-activity domain. To push the frontier of first-person video understanding of skilled human activity, we also present a suite of benchmark tasks and their annotations, including fine-grained activity understanding, proficiency estimation, cross-view translation, and 3D hand/body pose. All resources are open sourced to fuel new research in the community.
</div>

<br/>

<div class="m-text">CVPR 2024, Oral</div>

<div class="m-text">
<a href="https://arxiv.org/abs/2311.18259" class="m-link-wrap">Paper</a>, <a href="https://ego-exo4d-data.org/" class="m-link-wrap">Project</a>, <a href="https://docs.ego-exo4d-data.org" class="m-link-wrap">Code</a>
</div></div>
</div>
<div class="m-row m-block m-primary">
<div class="m-col-l-4">
<img alt="hssd" class="m-image" src="/images/papers/hssd.png" />
</div>
<div class="m-col-l-8">
<h3>Habitat Synthetic Scenes Dataset (HSSD-200): <br/>
 An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation</h3>

<div class="m-text">
<a href="https://mukulkhanna.github.io/">Mukul Khanna</a>*, Yongsen Mao*, <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, <a href="https://www.sanjayharesh.com/">Sanjay Haresh</a>, <a href="https://cs.stanford.edu/~bps/">Brennan Shacklett</a>, <a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a>, <a href="https://www.linkedin.com/in/alexander-clegg-68336839/">Alexander Clegg</a>, <a href="https://www.linkedin.com/in/ericu/">Eric Undersander</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://msavva.github.io/">Manolis Savva</a>
</div>
<br/>

<div class="m-text">
We contribute the Habitat Synthetic Scenes Dataset (HSSD-200), a dataset of 211 high-quality 3D scenes, and use it to test navigation agent generalization to realistic 3D environments. Our dataset represents real interiors and contains a diverse set of 18,656 models of real-world objects. We investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects (ObjectGoal navigation). By comparing to synthetic 3D scene datasets from prior work, we find that scale helps in generalization, but the benefits quickly saturate, making visual fidelity and correlation to real-world scenes more important. Our experiments show that agents trained on our smaller-scale dataset can match or outperform agents trained on much larger datasets. Surprisingly, we observe that agents trained on just 122 scenes from our dataset outperform agents trained on 10,000 scenes from the ProcTHOR-10K dataset in terms of zero-shot generalization in real-world scanned environments.
</div>

<br/>

<div class="m-text">CVPR 2024</div>

<div class="m-text">
<a href="https://arxiv.org/abs/2306.11290" class="m-link-wrap">Paper</a>, <a href="https://3dlg-hcvc.github.io/hssd/" class="m-link-wrap">Project</a>, <a href="https://github.com/3dlg-hcvc/hssd/" class="m-link-wrap">Code</a>
</div></div>
</div>
<div class="m-row m-block m-primary">
<div class="m-col-l-4">
<img alt="multiscan" class="m-image" src="/images/papers/multiscan.png" />
</div>
<div class="m-col-l-8">
<h3>MultiScan: Scalable RGBD scanning for 3D environments with articulated objects</h3>

<div class="m-text">
    Yongsen Mao, <a href="https://github.com/eamonn-zh/">Yiming Zhang</a>, <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>, <a href="https://msavva.github.io/">Manolis Savva</a>
</div>

<br/>
<div class="m-text">
    We introduce MultiScan, a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. We use this pipeline to collect 230 scans of 108 indoor scenes containing 9458 objects and 4331 parts. The resulting MultiScan dataset provides RGBD streams with per-frame camera poses, textured 3D surface meshes, richly annotated part-level and object-level semantic labels, and part mobility parameters. We validate our dataset on instance segmentation and part mobility estimation tasks and benchmark methods for these tasks from prior work. Our experiments show that part segmentation and mobility estimation in real 3D scenes remain challenging despite recent progress in 3D object segmentation.
</div>
<br/>

<div class="m-text">NeurIPS 2022</div>

<div class="m-text">
<a href="https://openreview.net/pdf?id=YxUdazpgweG" class="m-link-wrap">Paper</a>, <a href="https://3dlg-hcvc.github.io/multiscan/#/" class="m-link-wrap">Project</a>, <a href="https://github.com/smartscenes/multiscan" class="m-link-wrap">Code</a>
</div></div>
</div>
<div class="m-row m-block m-primary">
<div class="m-col-l-4">
<img alt="opd" class="m-image" src="/images/papers/opd.png" />
</div>
<div class="m-col-l-8">
<h3>OPD: Single-view 3D Openable Part Detection</h3>

<div class="m-text">
    <a href="https://jianghanxiao.github.io/">Hanxiao Jiang</a>, Yongsen Mao, <a href="https://msavva.github.io/">Manolis Savva</a>, <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
</div>

<br/>
<div class="m-text">
    We address the task of predicting what parts of an object can open and how they move when they do so. The input is a single image of an object, and as output we detect what parts of the object can open, and the motion parameters describing the articulation of each openable part. To tackle this task, we create two datasets of 3D objects: OPDSynth based on existing synthetic objects, and OPDReal based on RGBD reconstructions of real objects. We then design OPDRCNN, a neural architecture that detects openable parts and predicts their motion parameters. Our experiments show that this is a challenging task especially when considering generalization across object categories, and the limited amount of information in a single image. Our architecture outperforms baselines and prior work especially for RGB image inputs.
</div>
<br/>

<div class="m-text">ECCV 2022, Oral</div>

<div class="m-text">
<a href="https://arxiv.org/pdf/2203.16421.pdf" class="m-link-wrap">Paper</a>, <a href="https://3dlg-hcvc.github.io/OPD/" class="m-link-wrap">Project</a>, <a href="https://github.com/3dlg-hcvc/OPD" class="m-link-wrap">Code</a>
</div></div>
</div>
</section>
<!-- /content -->
      </div>
    </div>
  </div>
</article>
</main>
<footer><nav>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <p>Powered by <a href="https://getpelican.com">Pelican</a> and <a href="https://mcss.mosra.cz">m.css</a>.</p>
      </div>
    </div>
  </div>
</nav></footer>
</body>
</html>